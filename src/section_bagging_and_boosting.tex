\section{ Bagging and Boosting}\label{sec:bagging_and_boosting}
\begin{sectionbox}[Bagging and Subbagging]\nospacing{}
  \textbf{B}ootstrap \textbf{agg}regat\textbf{ing} (bagging) (mostly on trees), uses $\hat g(\cdot): \mathbb{R}^{p}\to \mathbb{R}$ and ensembles them (which comes at the loss of interpretability).
  \begin{enumeratenosep}[label=\roman*]
    \item Generate bootstrap sample $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{n}^{\ast}, Y_{n}^{\ast})$ and compute $\hat{g}^{\ast}_{i=1}(\cdot)$. Repeat $B$ times.
    \item Aggregate bootstrap estimates with $\hat{g}_{\textrm Bag}(\cdot) = B^{-1}\sum_{i=1}^{B} \hat{g}^{\ast}_{i}(\cdot) \approx \mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)]$.
  \end{enumeratenosep}
  Note that $\hat{g}_{\textrm Bag}(\cdot) = \hat{g}(\cdot) + \underbrace{(\mathbb{E}^{\ast}[\hat{g}^{\ast}(\cdot)] -\hat{g}(\cdot))}_{\text{bootstrap bias estimate}}$.
  We can reduce variance at price of higher bias (at least for trees).
  In fact, for many $x$, $Var(\hat{g}_{\textrm Bag}(x)) < Var(\hat{g}(x))$. We can use larger trees (higher variance) to balance the bias-variance trade-off.

  For \textbf{Sub}sample \textbf{agg}regat\textbf{ing} (Subbagging), we draw $(X_{1}^{\ast}, Y_{1}^{\ast}), \dots, (X_{m}^{\ast}, Y_{m}^{\ast})$ without replacement (e.g. with $m = \lfloor n/2\rfloor$), which can be cheaper overall and is equivalent to Bagging in some simple settings.
\end{sectionbox}

\begin{sectionbox}[AdaBoost.M1]\nospacing{}

  \begin{enumeratenosep}[label=\roman*]
\item Initialize the weights $w_{i}=1 / n, i=1, \ldots, n$.
\item For $m=1, \ldots, M$ :
\begin{enumeratenosep}[label=\alph*)]
    \item Fit a weak classifier $G_{m}(\boldsymbol{x})$ to training data using $w_{i}$'s.
\item Compute
$
\operatorname{err}_{m}=\frac{\sum_{i=1}^{n} w_{i} I\left\{y_{i} \neq G_{m}\left(\boldsymbol{x}_{i}\right)\right\}}{\sum_{i=1}^{n} w_{i}}
$
\item Compute $\alpha_{m}=\log \left\{\left(1-\operatorname{err}_{m}\right) / \operatorname{err}_{m}\right\}$.
\item Set $w_{i} \leftarrow w_{i} \times \exp \left[\alpha_{m} I\left\{y_{i} \neq G_{m}\left(\boldsymbol{x}_{i}\right)\right\}\right], i=1, \ldots, n$.
\end{enumeratenosep}

\item Output $G(\boldsymbol{x})=\operatorname{sign}\left\{\sum_{m=1}^{M} \alpha_{m} G_{m}(\boldsymbol{x})\right\}$.
\end{enumeratenosep}
\end{sectionbox}
\begin{notebox}[Misc]\nospacing{}
  \begin{enumeratenosep}[label=\roman*]
    \item RStudio: \texttt{Tools} $\rightarrow$ \texttt{Global Options} $\rightarrow$ \texttt{Code} $\rightarrow$ \texttt{Display}
    \item ROC: FPR ($x$) vs TPR ($y$)
    \item $\text{P}[>|t|]$: large ($>0.05)$ $\rightarrow$ $\beta_j \approx 0$ $\rightarrow$ $\beta_j$ not relevant; small $\rightarrow$ $\beta_j!=0$ $\rightarrow$ $\beta_j$ relevant \& reject $H_{j,0}$ (R: \texttt{pt(..)})
  \end{enumeratenosep}
  \begin{mintlinebox}{R}
    print(.., digits=..); table(df$Class, y.pred); residuals(..)
    factor(Class) # for classification; optim(..); termplot(..)
  \end{mintlinebox}
\end{notebox}
