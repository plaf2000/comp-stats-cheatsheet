\section{Classification}\label{sec:classification}
Given $(X_1, Y_1), \dots, (X_n,Y_n)$ i.i.d. with $Y_i \in \{0, \dots, J-1\}$, determine $\pi_j(x) = \mathbb{P}[Y=j\mid X=x]\ \forall j = 0,1,\dots,J-1$.
The optimal classifier is $\mathcal{C}_{\textrm Bayes}(x) = \argmax_{0\leq j\leq J-1}\pi_j(x)$.
Then Bayes risk for the 0-1-loss is $\mathbb{P}[\mathcal{C}_{\textrm Bayes}(X_{\textrm new}) \neq Y_{\textrm new}]$.

\begin{sectionbox}[Discriminant analysis]\nospacing{}
  \textbf{LDA: }Assume \mbox{$X \mid Y=j \sim \mathcal{N}(\mu_j, \Sigma), \mathbb{P}[Y=j] = p_j$,}.
  Then by Bayes formula $$\pi_j(x) = \frac{f_{X|Y=j}(x)\cdot p_j}{\sum_{k=0}^{J-1}f_{X|Y=k}(x)\cdot p_k}$$ with each $f_{X|Y=j}$ a Gaussian $\mathcal{N}(\mu_j, \Sigma)$.
  We can estimate $\mu_j$ and $\Sigma$ by the (closed-form) MLEs, and we also need a prior for $Y$, which often is fixed as $\hat p_j=n_j/n$.
  This results in $\hat \delta_j(x) = (x-\hat{\mu}_j/2)^{\top}\Sigma^{-1}\hat{\mu}_j+\log(\hat p_j)$ with linear (in $x$) decision boundaries $\hat{\delta}_j(x) = \hat \delta_{j'}(x)$ and $\mathcal{C}(x) = \argmax_j \hat \delta_j(x)$.

  \textbf{QDA: } Now we assume different $\Sigma_j$ for each class and obtain quadratic decision boundaries $\hat{\delta}_j(x) = -\log(\det(\hat\Sigma_j))/2 - (x-\hat{\mu}_j)^{\top}\hat{\Sigma}_j^{-1}(x-\hat{\mu}_j)/2 + \log(\hat p_j)$.
  The price: $J\cdot p(p+1)/2$ parameters (for all $\Sigma_j$) vs. $p(p+1)/2$ (symmetry of $\Sigma$) for a single $\Sigma$ ($\mu$ ($J\cdot p$) and priors ($J-1$) are the same),
    \begin{mintlinebox}{R}
    require(MASS)
    class_lda = lda(x=df[, c("x1", "x2")], grouping=df[, "y"])
  \end{mintlinebox}
\end{sectionbox}

\begin{sectionbox}[Logistic regression for binary classification]\nospacing{}
  Given some model $g: \mathbb{R}^p \to \mathbb{R}$ (e.g. a linear model) we can use the logistic transform $\pi \mapsto \log(\pi/(1-\pi))$ to get probabilities: $\log(\pi(x)/(1-\pi(x))) = g(x)$ and $\pi(x) = 1/(1+\exp{(-g(x))})$.
  This implies $Y_i \sim \text{Bernoulli}(\pi(x_i))$ (e.g. weighted coin flip). The likelihood is thus $\prod_{i=1}^n\pi(x_i)^{Y_i}(1-\pi(x_i))^{1-Y_i}$.
  We typically estimate $\vec{\beta}$ using gradient descent.
  As $n\to \infty$ we can asymptotically compute the standard errors $\widehat{s.e.}(\hat{\beta}_j)$ and t-test statistics $\hat\beta_j/\widehat{s.e.}(\hat\beta_j) \sim \mathcal{N}(0,1)$ (under $H_{0,j}: \beta_j=0)$.
  \begin{mintlinebox}{R}
    fit = glm(Y~., data=data, family="binomial")
    mean((predict(fit, type="response") > 0.5) == data$Y)
  \end{mintlinebox}
\end{sectionbox}
\begin{notebox}[Linear predictors]\nospacing{}
  Note that both \emph{LDA} and \emph{Logistic regression} are \emph{linear} in the prediction variables.
  For LDA that comes from the Gaussian assumption (i.e. ``linearization'' of the true distribution), for Logistic regression it comes from the linear log-odds function.
\end{notebox}
\begin{notebox}[Multiclass case ($J>2$)]\nospacing{}
  \begin{enumeratenosep}
    \item $J$ classes $\rightarrow$ $J$ binary variables: $\tilde \pi_j(x) = \frac{\hat pi_j(x)}{\sum_{j=0}^{J-1}\hat\pi_j(x)}$
    \item Using \emph{multinomial distribution} (parametric linear logistic) (see \verb!multinom!)
    \item ``Reference class'' $\log(\pi_j(x)/\pi_0(x)) = g_j(x)$
    \item Pairwise 1-vs-1, fitting ${J \choose 2}\cdot p$ parameters
    \item Exploiting ``ordered'' classes with proportional odds
  \end{enumeratenosep}
\end{notebox}
\begin{notebox}[Manually Calculating Predicted Class]\nospacing{}
    \begin{enumeratenosep}[label=\roman*]
    \item Predict class for $x$: $\argmin_{k\in[1,J]}\sum_{j=1}^J L(j,k)\cdot \pi_j(x)$
    \item E.g loss for predicting class $c: \sum_{j=1}^J L(j,c)\cdot \pi_j(x)$
  \end{enumeratenosep}
\end{notebox}
\begin{sectionbox}[RKHS]\nospacing{}
RKHS are Hilbert spaces generated by positive definite kernels, called \textbf{Mercer Kernels}. E.g. Mercel kernel: $K(\mathbf{x}_i,\mathbf{x}_j) = \mathbf\phi(\mathbf{x}_i)^\top\phi(\mathbf{x}_j)$



\end{sectionbox}
\begin{sectionbox}[Mercer theorem]
Any mercer kernel K admits eigen decomposition of the form
$K(\mathbf{x}, \mathbf{x}') = \sum_{j=1}^{\infty} \gamma_j \phi_j(\mathbf{x}) \phi_j(\mathbf{x}')$, where $\gamma_j$ are decreasingly ordered, $\sum_{j\geq 1}|\gamma_j|^2<\infty$ and ${\phi_j}_j$ is a set of orthonormal functions in $L^2$. The RKHS induced by kernel is $\mathcal{H}_K = \left\{ f : \mathbb{R}^p \to \mathbb{R} : f(\mathbf{x}) = \sum_{i=1}^{\infty} c_i \phi_i(\mathbf{x}), \ ||f||_{\mathcal{H}_K}^2=\sum_{i=1}^{\infty} \frac{c_i^2}{\gamma_i} < \infty \right\}$ The norm corresponds to the inner product $\langle f, h \rangle_{\mathcal{H}_K} = \sum_{i=1}^{\infty} \frac{c_i d_i}{\gamma_i}.
$, where $f =  \sum_{i=1}^{\infty} c_i\phi_i(\mathbf{x})$ and $g =  \sum_{i=1}^{\infty} d_i\phi_i(\mathbf{x})$.

\end{sectionbox}
\begin{sectionbox}[Representer Theorem]\nospacing{}
$\hat{f} = \min_{f\in \mathcal{H}_K}\frac1n\sum^n_{i=1}{L(y_i,f(x_i)) + \lambda ||{f}||_{\mathcal{H}_K}^2} = \sum_{i=1}^n\hat{\alpha_i}K_{x_i}(x)$, where $K_{x_i}(x)=K(x,x_i)$, so equivalent to the finite-dimensional problem $\min_{f\in \mathcal{H}_K}\frac1n\sum^n_{i=1}{L(y_i,\sum_{j=1}^n\alpha_jK_{x_j}(\mathbf{x}_i)) + \lambda \mathbf{\alpha}}^\top\mathbf{K}\mathbf{\alpha}$, where $\sum_{j=1}^n\alpha_jK_{x_j}(\mathbf{x}_i) = (\mathbf{K\alpha})_i$

E.g. squared loss regression: $\hat{\alpha} = \argmin_{\alpha}{\frac1n ||\mathbf{y} - \mathbf{K\alpha}||^2 + \mathbf{\lambda\alpha}^\top \mathbf{K\alpha}} = (\mathbf{K} + n\mathbf{\lambda I})^{-1}\mathbf{y}$

\end{sectionbox}
\begin{sectionbox}[SVM]\nospacing{}
$\min_{\mathbf{w}} \sum_{i=1}^{n} L(y_i, f(x_i)) + \frac{1}{2C}||\mathbf{w}\|^2$ with $f(\mathbf{x}) = \mathbf{w}^\top\mathbf{x} + b$ and hinge loss $L(y,f) = (1-yf)_+$ with $(u)_+ = max(u,0)$. Kernel SVM solution is of the form $\hat{\beta}_0 + \hat{f}(\mathbf{x}) = \hat{\beta}_0 + \sum_{i=1}^n{\hat\alpha_i}K(\mathbf{x},\mathbf{x}_i)$
\begin{mintlinebox}{R}
    require(e1071)
    tune.out <- tune(svm,Y~., data=dat, kernel="radial", ranges=list(cost=c(...), gamma=c(...)))
    summary (tune.out)
  \end{mintlinebox}
\end{sectionbox}